---
layout:					post
title:					"hadoop入门(一)"
author:					Zhou Zhongqing
header-style:				text
catalog:					true
tags:
		- Web
		- JavaScript
---
​
hadoop是什么？

What Is Apache Hadoop?
The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.

Apache hadoop 是什么？

Apache的Hadoop项目™®可靠的、可扩展的开发开源软件，分布式计算框架

hadoop为什么出现？

 在很多领域里面，在现在这个时代下面，很多公司产生的数据太多了，数据量太大了。用原来的技术去做，有种捉襟见肘的感觉，要么在性能上面，要么在速度上面遇到了瓶颈，这个时候需要新的技术来解决，我们能想到的，比如用高并发，1M的数据单机来处理就够了，如果1000M数据，一台机子一个节点就可能做不了。
可能考虑集群，分布式系统。但是分布式系统是很难编写的，要考虑的问题很多。线程，进程，网络通信等等很多问题。业务逻辑本来很简单，但因为考虑到分布式系统的协调问题，程序变得非常复杂，在传统的技术上花费太大。
比如 count()，order by的业务，几百兆MySQL可以搞定，几十TB，Mysql和Oracle都扛不住。

hadoop解决的问题

海量数据的存储（HDFS）
海量数据的分析（MapReduce）
资源管理调度（YARN）

作者：Doug Cutting

受Google三篇论文的启发(GFS、MapReduce、BigTable)

hadoop具体能干什么？

hadoop擅长日志分析，facebook就用Hive来进行日志分析，2009年时facebook就有非编程人员的30%的人使用HiveQL进行数据分析；淘宝搜索中的自定义筛选也使用的Hive；利用Pig还可以做高级的数据处理，包括Twitter、LinkedIn 上用于发现您可能认识的人，可以实现类似Amazon.com的协同过滤的推荐效果。淘宝的商品推荐也是！在Yahoo！的40%的Hadoop作业是用pig运行的，包括垃圾邮件的识别和过滤，还有用户特征建模。（2012年8月25新更新，天猫的推荐系统是hive，少量尝试mahout！）

怎样解决海量数据的存储？



假如来一个512M的文件，我们设置128M一块，会分成3个块分别存储到datanode，这样客服端来访问这个文件时，就是并发的访问了,负载分散了,没有任何性能下降,如果有其中的datanode宕机了，其他的datanode有这一个块的副本，副本默认备份3份,不影响正常使用。

HDFS的架构：

主从结构
主节点， namenode
从节点，有很多个: datanode
namenode负责：
接收用户操作请求
维护文件系统的目录结构
管理文件与block之间关系，block与datanode之间关系
datanode负责：
存储文件
文件被分成block存储在磁盘上
为保证数据安全，文件会有多个副本

怎样解决海量数据的计算？



hadoop 运算分为2个阶段第一个Map阶段 ，第二个阶段Reduce阶段，Map阶段分发到有这个块的机器在本地运行，Map的结果汇总到Reduce上进行累加的过程，能够并发运行就并发运行，Reduce程序对全局结果处理；Reduce也可以分组统计。这就是MapReduce的运算模型。

Hadoop的特点：

扩容能力（Scalable）：能可靠地（reliably）存储和处理千兆字节（PB）数据。

成本低（Economical）：可以通过普通机器组成的服务器群来分发以及处理数据。这些服务器群总计可达数千个节点

高效率（Efficient）：通过分发数据，hadoop可以在数据所在的节点上并行地（parallel）处理它们，这使得处理非常的快速。

可靠性（Reliable）：hadoop能自动地维护数据的多份副本，并且在任务失败后能自动地重新部署（redeploy）计算任务。

安装hadoop  http://blog.csdn.net/baidu_19473529/article/details/52994594



​